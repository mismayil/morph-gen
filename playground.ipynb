{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(de[VB]+[Polarity=Pos])([VN]-r[Derivation=AorNom]+[PersonNumber=A3sg]+[Possessive=Pnon]+[Case=Bare])([NOMP]-sH[Derivation=Sim]+[PersonNumber=A3sg]+Hn[Possessive=P2sg]+NDA[Case=Loc]+[Copula=PresCop]+YHm[PersonNumber=V1sg])',\n",
       " '(ders[NOMP]+[PersonNumber=A3sg]+Hn[Possessive=P2sg]+NDA[Case=Loc]+[Copula=PresCop]+YHm[PersonNumber=V1sg])',\n",
       " '(ders[NOMP]+[PersonNumber=A3sg]+SH[Possessive=P3sg]+NDA[Case=Loc]+[Copula=PresCop]+YHm[PersonNumber=V1sg])']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from turkish_morphology import decompose, analyze\n",
    "\n",
    "word = \"kitaplarımızdaki\"\n",
    "word = \"yapamadıklarımızdan\"\n",
    "sentence = \"Yazarın kitaplarını yayın hakkını elinde bulunduran Aziz Nesin Vakfı'ndan yapılan açıklamaya göre,ilki 1946 yılında basılan Aziz Nesin'in kitapları, o dönemden bu yana 10 milyon 780 bin adet basıldı ve dağıtıldı\"\n",
    "word = \"dersindeyim\"\n",
    "\n",
    "analyses = analyze.surface_form(word, use_proper_feature=False)\n",
    "decomposition = decompose.human_readable_analysis(analyses[0])\n",
    "\n",
    "analyses\n",
    "# decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Decomposition(root=ders, pos=NOMP, meta_morphemes=['SH', 'NDA', 'YHm'], morphemes=['i', 'nde', 'yim']),\n",
       " Decomposition(root=ders, pos=NOMP, meta_morphemes=['Hn', 'NDA', 'YHm'], morphemes=['in', 'de', 'yim']),\n",
       " Decomposition(root=de, pos=NOMP, meta_morphemes=['r', 'sH', 'Hn', 'NDA', 'YHm'], morphemes=['r', 'si', 'n', 'de', 'yim']),\n",
       " Decomposition(root=de, pos=NOMP, meta_morphemes=['r', 'sH', 'Hn', 'NDA', 'YHm'], morphemes=['r', 'si', 'n', 'de', 'yim'])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from morphology import decompose_tr\n",
    "\n",
    "decomposition = decompose_tr(word)\n",
    "decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/bilkent-turkish-writings/bilkent-turkish-writings-dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from utils import write_json\n",
    "\n",
    "essays = []\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    if isinstance(row[\"text\"], str):\n",
    "        raw_texts = re.split(\"[.?!]\\s\",row[\"text\"].replace(\"\\r\\n\", \"\"))\n",
    "        texts = []\n",
    "\n",
    "        for text in raw_texts:\n",
    "            texts.extend(re.split(\"\\s\\s\\s\\s\", text))\n",
    "\n",
    "        texts = [re.sub(\"^[A-ZÜÖĞIŞÇİ0-9\\-\\s.]+$\", \"\", text) for text in texts]\n",
    "        texts = [re.sub(\"\\s+\", \" \", text).strip() for text in texts if all([\"http\" not in text.lower(), \"kaynakça\" not in text.lower()]) and len(text.strip().split()) > 5]\n",
    "        \n",
    "        if len(texts) > 1:\n",
    "            essays.append({\n",
    "                \"id\": f\"bilkent-turkish-writings-{i}\",\n",
    "                \"sentences\": texts\n",
    "            })\n",
    "\n",
    "write_json(essays, \"data/bilkent-turkish-writings/bilkent-turkish-writings-dataset.json\", ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from morphology import decompose_tr\n",
    "\n",
    "# decompose_tr(\"heykelciklerinin\")\n",
    "[decomposition.to_json() for decomposition in decompose_tr(\"yansıttığımızın\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_json\n",
    "\n",
    "data = read_json(\"../data/bilkent-turkish-writings/bilkent-turkish-writings-dataset_prep.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data), sum([len(val) for val in data.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1 for root, word_map in data.items() for word, decompositions in word_map.items() if len(decompositions) == 1 and decompositions[0][\"morphemes\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import write_json\n",
    "import pathlib\n",
    "\n",
    "pathlib.Path(\"../data/bilkent-turkish-writings/batches\").mkdir(parents=True, exist_ok=True)\n",
    "i = 0\n",
    "while True:\n",
    "    data_chunk = list(data.items())[i*500:(i+1)*500]\n",
    "    if data_chunk:\n",
    "        write_json(dict(data_chunk), f\"../data/bilkent-turkish-writings/batches/bilkent-turkish-writings-dataset_prep_{i}.json\", ensure_ascii=False)\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import write_json\n",
    "from prepare_data_for_morph import post_morph_process\n",
    "\n",
    "new_data = post_morph_process(\"../data/bilkent-turkish-writings/batches/bilkent-turkish-writings-dataset_prep_0.json\")\n",
    "\n",
    "write_json(new_data, \"../data/bilkent-turkish-writings/batches/bilkent-turkish-writings-dataset_prep_0_new.json\", ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import read_json, write_json\n",
    "\n",
    "data = read_json(\"../data/bilkent-turkish-writings/btwd_prep.json\")\n",
    "\n",
    "output_data = {\n",
    "    \"metadata\": {\n",
    "        \"source\": \"../data/bilkent-turkish-writings/btwd.json\",\n",
    "        \"processor\": \"tr_btwd_prep\",\n",
    "        \"language\": \"tr\"\n",
    "    },\n",
    "    \"data\": data\n",
    "}\n",
    "write_json(output_data, \"../data/bilkent-turkish-writings/btwd_prep.json\", ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import read_json, write_json\n",
    "\n",
    "data = read_json(\"outputs/gpt-3.5/tr/btwd/btwd_prep_post_morph_sample_llm_morph_gen_en_one_shot_gpt-3.5-turbo_2094783b7ba0.json\")\n",
    "write_json(data, \"outputs/gpt-3.5/tr/btwd/btwd_prep_post_morph_sample_llm_morph_gen_en_one_shot_gpt-3.5-turbo_2094783b7ba0.json\", ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mismayil/.pyenv/versions/3.9.7/envs/til/lib/python3.9/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel(\"../data/MorphoLEX_en.xlsx\", sheet_name=\"0-1-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ELP_ItemID</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Nmorph</th>\n",
       "      <th>PRS_signature</th>\n",
       "      <th>MorphoLexSegm</th>\n",
       "      <th>ROOT1_PFMF</th>\n",
       "      <th>ROOT1_FamSize</th>\n",
       "      <th>ROOT1_Freq_HAL</th>\n",
       "      <th>SUFF1_PFMF</th>\n",
       "      <th>...</th>\n",
       "      <th>SUFF1_Freq_HAL</th>\n",
       "      <th>SUFF1_length</th>\n",
       "      <th>SUFF1_P</th>\n",
       "      <th>SUFF1_P*</th>\n",
       "      <th>SUFF2_PFMF</th>\n",
       "      <th>SUFF2_FamSize</th>\n",
       "      <th>SUFF2_Freq_HAL</th>\n",
       "      <th>SUFF2_length</th>\n",
       "      <th>SUFF2_P</th>\n",
       "      <th>SUFF2_P*</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>algebraical</td>\n",
       "      <td>JJ</td>\n",
       "      <td>3</td>\n",
       "      <td>0,1,2</td>\n",
       "      <td>{(algebra)}&gt;ic&gt;&gt;al&gt;</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3692</td>\n",
       "      <td>88.450148</td>\n",
       "      <td>...</td>\n",
       "      <td>1472797</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.006334</td>\n",
       "      <td>91.538462</td>\n",
       "      <td>1431</td>\n",
       "      <td>4704731</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.006643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>allowably</td>\n",
       "      <td>RB</td>\n",
       "      <td>3</td>\n",
       "      <td>0,1,2</td>\n",
       "      <td>{(allow)}&gt;able&gt;&gt;y&gt;</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5</td>\n",
       "      <td>185956</td>\n",
       "      <td>88.978186</td>\n",
       "      <td>...</td>\n",
       "      <td>1227992</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>73.641851</td>\n",
       "      <td>2486</td>\n",
       "      <td>3870233</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.029816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69</td>\n",
       "      <td>allusively</td>\n",
       "      <td>RB</td>\n",
       "      <td>3</td>\n",
       "      <td>0,1,2</td>\n",
       "      <td>{(allude)}&gt;ive&gt;&gt;ly&gt;</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2066</td>\n",
       "      <td>92.746114</td>\n",
       "      <td>...</td>\n",
       "      <td>1037354</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>80.980324</td>\n",
       "      <td>2898</td>\n",
       "      <td>3857999</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.026726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94</td>\n",
       "      <td>alphabetically</td>\n",
       "      <td>RB</td>\n",
       "      <td>3</td>\n",
       "      <td>0,1,2</td>\n",
       "      <td>{(alphabet)}&gt;al&gt;&gt;ly&gt;</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6877</td>\n",
       "      <td>30.279720</td>\n",
       "      <td>...</td>\n",
       "      <td>4704731</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.006643</td>\n",
       "      <td>13.496721</td>\n",
       "      <td>2898</td>\n",
       "      <td>3857999</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.026726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>116</td>\n",
       "      <td>altercations</td>\n",
       "      <td>NN</td>\n",
       "      <td>3</td>\n",
       "      <td>0,1,2</td>\n",
       "      <td>{(alterc)&gt;ate&gt;}&gt;ion&gt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>56.623932</td>\n",
       "      <td>...</td>\n",
       "      <td>2569118</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.010505</td>\n",
       "      <td>58.322904</td>\n",
       "      <td>1599</td>\n",
       "      <td>6530204</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.009115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ELP_ItemID            Word POS  Nmorph PRS_signature         MorphoLexSegm  \\\n",
       "0           4     algebraical  JJ       3         0,1,2   {(algebra)}>ic>>al>   \n",
       "1          61       allowably  RB       3         0,1,2    {(allow)}>able>>y>   \n",
       "2          69      allusively  RB       3         0,1,2   {(allude)}>ive>>ly>   \n",
       "3          94  alphabetically  RB       3         0,1,2  {(alphabet)}>al>>ly>   \n",
       "4         116    altercations  NN       3         0,1,2  {(alterc)>ate>}>ion>   \n",
       "\n",
       "   ROOT1_PFMF  ROOT1_FamSize  ROOT1_Freq_HAL  SUFF1_PFMF  ...  SUFF1_Freq_HAL  \\\n",
       "0       100.0              4            3692   88.450148  ...         1472797   \n",
       "1       100.0              5          185956   88.978186  ...         1227992   \n",
       "2       100.0              4            2066   92.746114  ...         1037354   \n",
       "3        50.0              5            6877   30.279720  ...         4704731   \n",
       "4         0.0              2             187   56.623932  ...         2569118   \n",
       "\n",
       "   SUFF1_length   SUFF1_P  SUFF1_P*  SUFF2_PFMF  SUFF2_FamSize  \\\n",
       "0             2  0.000028  0.006334   91.538462           1431   \n",
       "1             4  0.000032  0.006025   73.641851           2486   \n",
       "2             3  0.000017  0.002781   80.980324           2898   \n",
       "3             2  0.000009  0.006643   13.496721           2898   \n",
       "4             3  0.000026  0.010505   58.322904           1599   \n",
       "\n",
       "   SUFF2_Freq_HAL  SUFF2_length   SUFF2_P  SUFF2_P*  \n",
       "0         4704731             2  0.000009  0.006643  \n",
       "1         3870233             1  0.000050  0.029816  \n",
       "2         3857999             2  0.000045  0.026726  \n",
       "3         3857999             2  0.000045  0.026726  \n",
       "4         6530204             3  0.000009  0.009115  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    word = row[\"Word\"]\n",
    "    segmentation = row[\"MorphoLexSegm\"]\n",
    "    morphemes = re.findall(r\"[A-Za-z]+\", segmentation)\n",
    "    if word == \"\".join(morphemes):\n",
    "        words.append({\n",
    "            \"word\": word,\n",
    "            \"morphemes\": morphemes\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'algebraical', 'morphemes': ['algebra', 'ic', 'al']},\n",
       " {'word': 'altruistic', 'morphemes': ['altru', 'ist', 'ic']},\n",
       " {'word': 'amateurishly', 'morphemes': ['amateur', 'ish', 'ly']},\n",
       " {'word': 'ambassadorship', 'morphemes': ['ambassad', 'or', 'ship']},\n",
       " {'word': 'ambitiousness', 'morphemes': ['ambit', 'ious', 'ness']},\n",
       " {'word': 'woodenly', 'morphemes': ['wood', 'en', 'ly']},\n",
       " {'word': 'woodenness', 'morphemes': ['wood', 'en', 'ness']},\n",
       " {'word': 'wordlessly', 'morphemes': ['word', 'less', 'ly']},\n",
       " {'word': 'workableness', 'morphemes': ['work', 'able', 'ness']},\n",
       " {'word': 'worthlessly', 'morphemes': ['worth', 'less', 'ly']}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['algebra', 'ic', 'al']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "s = \"{(algebra)}>ic>>al>\"\n",
    "matches = re.findall(r\"[A-Za-z]+\", s)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import read_json, write_json\n",
    "\n",
    "data = read_json(\"experiments/data/en/morpholex/MorphoLEX_en_prep_morph_sample_nonce.json\")\n",
    "\n",
    "for i, sample in enumerate(data[\"data\"]):\n",
    "    sample[\"id\"] = f\"morpholex-ood-{i}\"\n",
    "\n",
    "write_json(data, \"experiments/data/en/morpholex/MorphoLEX_en_prep_morph_sample_nonce.json\", ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import read_json, write_json\n",
    "\n",
    "data = read_json(\"experiments/data/tr/btwd/btwd_prep_post_comp_morph_sample_nonce.json\")\n",
    "\n",
    "for i, sample in enumerate(data[\"data\"]):\n",
    "    sample[\"id\"] = f\"tr-ood-{i}\"\n",
    "\n",
    "write_json(data, \"experiments/data/tr/btwd/btwd_prep_post_comp_morph_sample_nonce.json\", ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "til",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
